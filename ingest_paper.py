# ingest_papers.py
# ============================================================
# PDF -> GPT(ë©”íƒ€/ë³¸ë¬¸/í•œì¤„ìš”ì•½) -> Notion DB ê¸°ë¡
# + Notion file_uploads RESTë¡œ PDF/ì´ë¯¸ì§€ ì§ì ‘ ì—…ë¡œë“œ
# + ë…¼ë¬¸ URL Bookmark + (ì„ê¸° ë ˆì´ì•„ì›ƒ) Sections & Figures + Contents
# ============================================================

# 0) Imports
import os
import re
import glob
import json
import sys
import io
import mimetypes
from datetime import datetime
from urllib.parse import quote_plus

import requests
from dotenv import load_dotenv
from pypdf import PdfReader
from notion_client import Client as NotionClient
from notion_client.errors import APIResponseError
from openai import OpenAI

try:
    import fitz  # PyMuPDF
except ImportError:
    fitz = None
    
# ìƒë‹¨ì— ì¶”ê°€í• ì§€ ì—¬ë¶€ (ì „ì—­)
PROMOTE_FIRST_FIGURE = True

def pick_first_figure_id(figures: list[dict]) -> str | None:
    """
    ê°€ì¥ ì´ë¥¸ figureë¥¼ ì„ íƒ (page ì˜¤ë¦„ì°¨ìˆœ, ë™ì ì´ë©´ id ì‚¬ì „ìˆœ)
    """
    if not figures:
        return None
    return sorted(figures, key=lambda f: (int(f.get("page", 10**9) or 10**9), str(f.get("id",""))))[0].get("id")


# 1) Env / Clients
load_dotenv()
NOTION_TOKEN   = os.getenv("NOTION_TOKEN")
DATABASE_ID    = os.getenv("NOTION_DATABASE_ID")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL   = os.getenv("OPENAI_MODEL", "chatgpt-4o-latest")
NOTION_VERSION = "2022-06-28"

OPENAI_MAX_TOKENS = int(os.getenv("OPENAI_MAX_TOKENS", "700"))
_SURROGATE_RE = re.compile(r"[\ud800-\udfff]")


def strip_unpaired_surrogates(s: str) -> str:
    """
    ë¬¸ìì—´ì—ì„œ ê³ ì•„ ì„œëŸ¬ê²Œì´íŠ¸ë¥¼ ì œê±°í•œë‹¤.
    - ì •ìƒì ì¸ UTF-16 ìŒì€ ë³´ì¡´ë¨ (encode/decode íŠ¸ë¦­)
    """
    if not isinstance(s, str):
        return s
    # 1) ë¹ ë¥¸ ê²½ë¡œ: ëª…ì‹œì  ì œê±°(ê³ ì•„ í¬í•¨ ì˜ì—­ ì „ë¶€ ì‚­ì œ)
    s = _SURROGATE_RE.sub("", s)
    # 2) ë³´ìˆ˜ì  ë³µì›: ìœ íš¨í•œ ìŒì€ ìœ ì§€, ê³ ì•„ëŠ” drop
    #   (surrogatepassë¡œ 16ë¹„íŠ¸ ë‹¨ìœ„ ê·¸ëŒ€ë¡œ ì¸ì½”ë“œ â†’ ìœ íš¨í•˜ì§€ ì•Šì€ ì¡°í•©ë§Œ decodeì—ì„œ ì†Œê±°)
    try:
        s = s.encode("utf-16", "surrogatepass").decode("utf-16", "ignore")
    except Exception:
        # í™˜ê²½ì— ë”°ë¼ ìœ„ ë‹¨ê³„ê°€ ë¶ˆí¸í•˜ë©´, utf-8 ignore fallback
        s = s.encode("utf-8", "ignore").decode("utf-8", "ignore")
    # ë„ë¬¸ì ë“± í†µì‹ ì— ê±°ìŠ¬ë¦¬ëŠ” ì œì–´ë¬¸ìë„ ì •ë¦¬
    s = s.replace("\x00", "")
    return s

def sanitize_text(obj):
    """
    dict/list/str ì¬ê·€ ìˆœíšŒí•˜ë©´ì„œ ëª¨ë“  ë¬¸ìì—´ì— strip_unpaired_surrogates ì ìš©
    """
    if isinstance(obj, str):
        return strip_unpaired_surrogates(obj)
    if isinstance(obj, list):
        return [sanitize_text(x) for x in obj]
    if isinstance(obj, dict):
        return {k: sanitize_text(v) for k, v in obj.items()}
    return obj

if not (NOTION_TOKEN and DATABASE_ID and OPENAI_API_KEY):
    print("âŒ .envì— NOTION_TOKEN / NOTION_DATABASE_ID / OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    sys.exit(1)

notion = NotionClient(auth=NOTION_TOKEN)
oai    = OpenAI(api_key=OPENAI_API_KEY)


# 2) Parameters (Figure íƒì§€/í¬ë¡­)
SCALE      = 2.0
MARGIN     = 6.0
MAX_VGAP   = 120.0
MIN_X_OVERLAP_RATIO = 0.20
MERGE_NEAR_IMAGES   = True
SAVE_LOCAL_FIGS     = False  # Trueë©´ _figures/ ì €ì¥ë„ ìˆ˜í–‰

CAPTION_RE = re.compile(
    r"^\s*(?:"
    r"(?:Figure|Fig\.?)\s*[0-9IVX]+|"
    r"(?:ê·¸ë¦¼|ë„)\s*\d+"
    r")\b",
    re.IGNORECASE
)

SECTION_KEYS = [
    "1. ê¸°ì¡´ë¬¸ì œ",
    "2. ì„ í–‰ì—°êµ¬",
    "3. ì´ë²ˆ ì—°êµ¬ì˜ ê°œì„ ì ",
    "4. ë¬¸ì œì˜ ì¤‘ìš”ì„±",
    "5. ì œì•ˆ ì‹œìŠ¤í…œ/ë°©ë²•",
    "6. ì‹¤í—˜ ê°€ì„¤/ì ˆì°¨",
]


# 3) Utilities
def s(val) -> str:
    return "" if val is None else str(val)

def guess_mime(path: str, default="application/octet-stream"):
    mime, _ = mimetypes.guess_type(path)
    return mime or default

def ensure_title_prop_name(dbid: str) -> str:
    db = notion.databases.retrieve(dbid)
    for k, v in db["properties"].items():
        if v["type"] == "title":
            return k
    raise RuntimeError("Notion DBì— title ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤.")

def read_pdf(path: str, max_chars: int = 40000) -> str:
    try:
        reader = PdfReader(path)
        texts = []
        for page in reader.pages:
            chunk = page.extract_text() or ""
            if chunk:
                texts.append(chunk)
            if sum(len(t) for t in texts) >= max_chars:
                break
        text = "\n\n".join(texts)
        text = re.sub(r"[ \t]+", " ", text)
        text = re.sub(r"\n{3,}", "\n\n", text).strip()
        if len(text) > max_chars:
            text = text[:max_chars]
        return text
    except Exception as e:
        return f"[PDF READ ERROR] {e}"

def safe_year_guess(filename: str, text: str) -> str:
    cand = re.findall(r"\b(19[7-9]\d|20[0-4]\d|2025|2026)\b", filename + " " + text)
    return cand[0] if cand else ""

def to_tag_list(val) -> list[dict]:
    if val is None:
        return []
    if isinstance(val, str):
        parts = [x.strip() for x in val.split(",") if x.strip()]
    elif isinstance(val, (list, tuple, set)):
        parts = [s(x).strip() for x in val if s(x).strip()]
    else:
        parts = [s(val).strip()]
    return [{"name": x} for x in parts]

def _heading(text: str, level: int = 2) -> dict:
    key = {1: "heading_1", 2: "heading_2", 3: "heading_3"}[max(1, min(level, 3))]
    return {"object": "block", "type": key, key: {"rich_text": [{"type": "text", "text": {"content": text}}]}}

def _paragraph_rich(parts: list[dict]) -> dict:
    return {"object":"block","type":"paragraph","paragraph":{"rich_text":parts}}

def _bookmark(url: str) -> dict:
    return {"object": "block", "type": "bookmark", "bookmark": {"url": url}}

def _numbered_rich(parts: list[dict]) -> dict:
    return {"object":"block","type":"numbered_list_item","numbered_list_item":{"rich_text":parts}}

def _notion_headers(extra: dict | None = None) -> dict:
    base = {"Authorization": f"Bearer {NOTION_TOKEN}", "Notion-Version": NOTION_VERSION}
    if extra: base.update(extra)
    return base


# 4) Notion file_uploads
def create_file_upload(filename: str, content_type: str) -> dict:
    url = "https://api.notion.com/v1/file_uploads"
    payload = {"filename": filename, "content_type": content_type}
    r = requests.post(url, headers=_notion_headers({"Content-Type":"application/json"}), data=json.dumps(payload), timeout=30)
    r.raise_for_status()
    return r.json()

def send_file_upload_bytes(file_upload_id: str, data: bytes, filename: str, content_type: str) -> dict:
    url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
    files = {"file": (filename, io.BytesIO(data), content_type)}
    r = requests.post(url, headers=_notion_headers(), files=files, timeout=120)
    r.raise_for_status()
    return r.json()

def send_file_upload_path(file_upload_id: str, file_path: str, content_type: str) -> dict:
    url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
    with open(file_path, "rb") as f:
        files = {"file": (os.path.basename(file_path), f, content_type)}
        r = requests.post(url, headers=_notion_headers(), files=files, timeout=120)
    r.raise_for_status()
    return r.json()

def file_block_from_upload(file_upload_id: str) -> dict:
    return {"object":"block","type":"file","file":{"type":"file_upload","file_upload":{"id":file_upload_id}}}

def image_block_from_upload(file_upload_id: str, caption: str = "") -> dict:
    block = {"object":"block","type":"image","image":{"type":"file_upload","file_upload":{"id":file_upload_id}}}
    if caption:
        block["image"]["caption"] = [{"type":"text","text":{"content":caption}}]
    return block


# 5) Paper URL finder
def find_paper_url(title: str, authors: str = "", year: str = "") -> str:
    if not title: return ""
    title_q = quote_plus(title.strip())
    # Crossref
    try:
        url = f"https://api.crossref.org/works?query.title={title_q}&rows=3"
        r = requests.get(url, timeout=10)
        if r.ok:
            items = r.json().get("message", {}).get("items", [])
            title_norm = re.sub(r"\s+"," ",title).strip().lower()
            best = None
            for it in items:
                it_title = " ".join(it.get("title", [])).strip()
                it_norm = re.sub(r"\s+"," ",it_title).strip().lower()
                score = 2 if it_norm == title_norm else (1 if title_norm in it_norm or it_norm in title_norm else 0)
                if year and str(it.get("issued", {}).get("date-parts", [[None]])[0][0]) == year: score += 1
                if not best or score > best[0]: best = (score, it)
            if best and best[0] >= 1:
                it = best[1]
                doi = it.get("DOI")
                if doi: return f"https://doi.org/{doi}"
                link = it.get("URL")
                if link: return link
    except Exception: pass
    # arXiv
    try:
        r = requests.get(f"http://export.arxiv.org/api/query?search_query=ti:{title_q}", timeout=10)
        if r.ok and "<entry>" in r.text:
            m = re.search(r"<id>(https?://arxiv\.org/abs/[^\<]+)</id>", r.text)
            if m: return m.group(1).strip()
    except Exception: pass
    # Semantic Scholar
    try:
        s2 = f"https://api.semanticscholar.org/graph/v1/paper/search?query={title_q}&limit=1&fields=url,externalIds,title,year"
        r = requests.get(s2, timeout=10)
        if r.ok and r.json().get("data"):
            d0 = r.json()["data"][0]
            if d0.get("url"): return d0["url"]
            ext = d0.get("externalIds", {})
            if "DOI" in ext: return f"https://doi.org/{ext['DOI']}"
    except Exception: pass
    return ""


# 6) Minimal Markdown(êµµê²Œ/ë°‘ì¤„/ì´íƒ¤ë¦­) â†’ Notion rich_text ë³€í™˜
MD_PATTERNS = [
    (re.compile(r"\*\*(.+?)\*\*"), {"bold": True}),
    (re.compile(r"__(.+?)__"),     {"underline": True}),
    (re.compile(r"\*(.+?)\*"),     {"italic": True}),
]

def md_to_rich_text(md: str) -> list[dict]:
    """
    **êµµê²Œ**, __ë°‘ì¤„__, *ì´íƒ¤ë¦­* ë§Œ ì§€ì›í•˜ëŠ” ì•„ì£¼ ì‘ì€ ë³€í™˜ê¸°
    ë‚˜ë¨¸ì§€ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
    """
    spans = [(0, md, {})]
    for pat, anno in MD_PATTERNS:
        new_spans = []
        for start_idx, text, attrs in spans:
            pos = 0
            for m in pat.finditer(text):
                if m.start() > pos:
                    new_spans.append((0, text[pos:m.start()], attrs))
                new_spans.append((0, m.group(1), {**attrs, **anno}))
                pos = m.end()
            if pos < len(text):
                new_spans.append((0, text[pos:], attrs))
        spans = new_spans

    rich = []
    for _, piece, attrs in spans:
        if not piece:
            continue
        rich.append({
            "type": "text",
            "text": {"content": piece},
            "annotations": {
                "bold": attrs.get("bold", False),
                "italic": attrs.get("italic", False),
                "underline": attrs.get("underline", False),
                "code": False, "strikethrough": False, "color": "default"
            }
        })
    return rich


# 7) GPT Calls
def call_gpt_meta(text_snippet: str) -> dict:
    text_snippet = strip_unpaired_surrogates(text_snippet)
    system_msg = "ë„ˆëŠ” ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œì— íŠ¹í™”ëœ ë¹„ì„œë‹¤. ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•´ë¼."
    user_msg = f"""ë‹¤ìŒ ë…¼ë¬¸ ìŠ¤ë‹ˆí«ì„ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ í‚¤ë§Œ í¬í•¨í•œ JSONì„ ë§Œë“¤ì–´ë¼.
- title
- authors
- year
- conference_journal
- tag

tag ìƒì„± ê·œì¹™:
1) ìŠ¤ë‹ˆí«ì˜ "CCS Concepts" ë˜ëŠ” ìœ ì‚¬ ì„¹ì…˜ ì•„ë˜ì— ìˆëŠ” í‚¤ì›Œë“œ/ì£¼ì œ í‘œí˜„ì„ 1~6ê°œ ì¶”ì¶œí•œë‹¤.
   - í˜•ì‹ì˜ ì°¨ì´ëŠ” ë¬´ì‹œí•˜ê³  í•µì‹¬ ëª…ì‚¬êµ¬ë¡œë§Œ ë‹´ëŠ”ë‹¤.
   - ì—†ìœ¼ë©´ ë…¼ë¬¸ ì£¼ì œì— ê¸°ë°˜í•´ CCS ìŠ¤íƒ€ì¼ì˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ 1~6ê°œ ì¶”ì •í•´ë¼.
2) ì´ì–´ì„œ ë…¼ë¬¸ì„ ë¹ ë¥´ê²Œ ë¶„ë¥˜í•˜ëŠ” ë° ìœ ìš©í•œ Tag ë¥¼ ì¶”ê°€í•´ë¼.
   - ì¤‘ë³µ ê¸ˆì§€
3) ê²°ê³¼ì ìœ¼ë¡œ tagëŠ” ["<CCS ë˜ëŠ” ì¶”ì •>", ..., "<ë¶„ë¥˜íƒœê·¸1>", "<ë¶„ë¥˜íƒœê·¸2>", "<ë¶„ë¥˜íƒœê·¸3>"] í˜•íƒœì˜ **í‰íƒ„ ë°°ì—´**ì´ì–´ì•¼ í•œë‹¤.
4) ëª¨ë“  ê°’ì€ í…ìŠ¤íŠ¸ë§Œ í¬í•¨. ì„¤ëª…/ê´„í˜¸/ê¸°í˜¸ëŠ” ë¹¼ê³  í•µì‹¬ì–´ë§Œ ë„£ì–´ë¼.

ìŠ¤ë‹ˆí«:
\"\"\"{text_snippet}\"\"\""""
    resp = oai.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[{"role":"system","content":system_msg},{"role":"user","content":user_msg}],
        temperature=0.2, response_format={"type":"json_object"}, max_tokens=OPENAI_MAX_TOKENS,
    )
    raw = resp.choices[0].message.content
    try: return json.loads(raw)
    except Exception: return {"title":"","authors":"","year":"","conference_journal":"","tag":[],"_raw":raw}

def call_gpt_contents_marked(text_snippet: str) -> dict:
    text_snippet = strip_unpaired_surrogates(text_snippet)
    """
    6ê°œ ì„¹ì…˜ì„ 'ì •í™•í•œ í‚¤ ì´ë¦„'ìœ¼ë¡œ ë°˜í™˜ + ë³¸ë¬¸ì€ 'ê°„ë‹¨ ë§ˆí¬ë‹¤ìš´' í¬í•¨ í—ˆìš©.
    ë˜í•œ ê° ì„¹ì…˜ ì²« ë¬¸ì¥ì€ 'ì œëª© : ' í˜•íƒœë¡œ ì‹œì‘í•˜ë„ë¡ ì§€ì‹œ.
    """
    system_msg = "ë„ˆëŠ” ë…¼ë¬¸ ìš”ì•½ì— íŠ¹í™”ëœ í•œêµ­ì–´ ì—°êµ¬ ë¹„ì„œë‹¤. ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•´ë¼."
    user_msg = f"""
ë‹¤ìŒ ë…¼ë¬¸ ìŠ¤ë‹ˆí«ì„ ë°”íƒ•ìœ¼ë¡œ 'contents' ë”•ì…”ë„ˆë¦¬ë¥¼ ë§Œë“¤ì–´ë¼.
í‚¤ëŠ” ë°˜ë“œì‹œ ì•„ë˜ 6ê°œë¥¼ ì •í™•íˆ ì‚¬ìš©í•œë‹¤(ì² ì/êµ¬ë‘ì  í¬í•¨, ìˆœì„œ ìœ ì§€):
- "1. ê¸°ì¡´ë¬¸ì œ"
- "2. ì„ í–‰ì—°êµ¬"
- "3. ì´ë²ˆ ì—°êµ¬ì˜ ê°œì„ ì "
- "4. ë¬¸ì œì˜ ì¤‘ìš”ì„±"
- "5. ì œì•ˆ ì‹œìŠ¤í…œ/ë°©ë²•"
- "6. ì‹¤í—˜ ê°€ì„¤/ì ˆì°¨"

ê° ê°’ì€ í•œêµ­ì–´ ë‹¨ë½ ë¬¸ìì—´ì´ë©°, ë‹¤ìŒ ê·œì¹™ì„ ì§€í‚¨ë‹¤:
- ì²« ë¬¸ì¥ì€ 'í‚¤ ì´ë¦„ : ' ìœ¼ë¡œ ì‹œì‘í•œë‹¤. ì˜ˆ) "1. ê¸°ì¡´ë¬¸ì œ : ..."
- ì½ê¸° ì¢‹ê²Œ **êµµê²Œ**, __ë°‘ì¤„__, *ì´íƒ¤ë¦­* ê°™ì€ ë§ˆí¬ë‹¤ìš´ì„ ì ì ˆíˆ ì‚¬ìš©í•œë‹¤.
- ê° ì„¹ì…˜ ê¸¸ì´ëŠ” ë„¤ê°€ ê°€ì§„ ì •ë³´ë¡œ ì¶©ë¶„íˆ ìš”ì•½í•˜ë˜, í•µì‹¬ ê°œë…ì—ëŠ” êµµê²Œë‚˜ ë°‘ì¤„ì„ ì‚¬ìš©í•œë‹¤.

ìŠ¤ë‹ˆí«:
\"\"\"{text_snippet}\"\"\""""

    resp = oai.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[{"role":"system","content":system_msg},{"role":"user","content":user_msg}],
        temperature=0.2, response_format={"type":"json_object"}, max_tokens=2000,
    )
    raw = resp.choices[0].message.content
    data = json.loads(raw)
    # contentsê°€ ì—†ëŠ” ê²½ìš° ë³´ì •
    if "contents" in data:
        contents = data["contents"]
    else:
        contents = data
    # ëˆ„ë½ í‚¤ ë³´ê°•
    for k in SECTION_KEYS:
        contents.setdefault(k, "")
    return contents

def call_gpt_layout(contents: dict, figures: list[dict]) -> list[dict]:
    """
    ì„¹ì…˜(1~6)ì„ ë°˜ë“œì‹œ ê·¸ ìˆœì„œëŒ€ë¡œ ëª¨ë‘ í¬í•¨í•˜ê³ ,
    ê° ì„¹ì…˜ ë’¤ì— ê´€ë ¨ í”¼ê²¨ë“¤ì„ ë°°ì¹˜í•˜ëŠ” ë ˆì´ì•„ì›ƒì„ LLMìœ¼ë¡œ ìƒì„±.
    ë°˜í™˜ ì˜ˆ:
      [{"type":"section","key":"1. ê¸°ì¡´ë¬¸ì œ"},
       {"type":"figure","id":"p01_f02"},
       {"type":"section","key":"2. ì„ í–‰ì—°êµ¬"},
       ...]
    """
    # 1) brief ìƒì„± (ê¸¸ì´ ì œí•œ + sanitize)
    def _brief(s: str, n: int = 600) -> str:
        return strip_unpaired_surrogates((s or "")[:n])

    # ì„¹ì…˜ ìš”ì•½(ë„ˆë¬´ ê¸¸ë©´ ì˜ë¦¬ë„ë¡)
    contents_brief = {k: _brief(contents.get(k, ""), 900) for k in SECTION_KEYS}

    # í”¼ê²¨ ìš”ì•½(ìº¡ì…˜ ê¸¸ì´ ì œí•œ)
    fig_brief = []
    for f in figures:
        fig_brief.append({
            "id": f.get("id"),
            "page": int(f.get("page", 0) or 0),
            "caption": _brief(f.get("caption_text", ""), 400),
        })

    # 2) í”„ë¡¬í”„íŠ¸
    system_msg = "ë„ˆëŠ” ë…¼ë¬¸ í¸ì§‘ ë ˆì´ì•„ì›ƒ ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ë¼."
    user_msg = f"""
ì•„ë˜ 'ì„¹ì…˜ ìš”ì•½'ê³¼ 'í”¼ê²¨ ëª©ë¡'ì„ ë°”íƒ•ìœ¼ë¡œ, ë…ìê°€ ì½ê¸° ìì—°ìŠ¤ëŸ¬ìš´ ë ˆì´ì•„ì›ƒ(JSON ë°°ì—´)ì„ ë§Œë“¤ì–´ë¼.

í•„ìˆ˜ ê·œì¹™:
1) 6ê°œ ì„¹ì…˜ì„ ì´ ìˆœì„œë¡œ ëª¨ë‘ í¬í•¨(ëˆ„ë½/ìˆœì„œ ë³€ê²½ ê¸ˆì§€): {SECTION_KEYS}
2) í”¼ê²¨ëŠ” **ê´€ë ¨ì„±ì´ ê°€ì¥ ë†’ì€ ì„¹ì…˜ ë°”ë¡œ ë’¤ì—** ë°°ì¹˜í•œë‹¤. 
   - ì„¹ì…˜ë“¤ ì‚¬ì´ì‚¬ì´ì— ìì—°ìŠ¤ëŸ½ê²Œ ë¶„ì‚° ë°°ì¹˜í•˜ê³ , 
   - **ëª¨ë“  í”¼ê²¨ë¥¼ ë§ˆì§€ë§‰ì— ëª°ì•„ì„œ ë°°ì¹˜í•´ì„œëŠ” ì•ˆ ëœë‹¤.**
3) ê° ì„¹ì…˜ í•­ëª©ì€ ì •í™•íˆ ë‹¤ìŒ í˜•íƒœë‹¤:
   {{"type":"section","key":"<ì„¹ì…˜í‚¤>"}}
4) í”¼ê²¨ í•­ëª©ì€ ë‹¤ìŒ í˜•íƒœë‹¤:
   {{"type":"figure","id":"<í”¼ê²¨ id>"}}
5) í”¼ê²¨ëŠ” **ê´€ë ¨ì„±ì´ ê°€ì¥ ë†’ì€ ì„¹ì…˜ ë°”ë¡œ ë’¤**ì— ë°°ì¹˜í•œë‹¤. (ì—¬ëŸ¬ ê°œë©´ ê·¸ ì„¹ì…˜ ë’¤ì— ì—°ë‹¬ì•„ ë°°ì¹˜)
   - ì˜ˆ: ë°©ë²•/ì•„í‚¤í…ì²˜ ë„ì‹ â†’ "5. ì œì•ˆ ì‹œìŠ¤í…œ/ë°©ë²•" ë’¤
   - ì‹¤í—˜ ê²°ê³¼ ê·¸ë˜í”„ â†’ "6. ì‹¤í—˜ ê°€ì„¤/ì ˆì°¨" ë’¤ (ë˜ëŠ” ê²°ê³¼/í‰ê°€ ë‚´ìš© ë’¤)
6) ê°™ì€ í”¼ê²¨ idëŠ” í•œ ë²ˆë§Œ ì‚¬ìš©í•œë‹¤.
7) ì˜¤ì§ JSON ë°°ì—´ë§Œ ì¶œë ¥í•œë‹¤. ì„¤ëª…/ì£¼ì„ ê¸ˆì§€.

ì„¹ì…˜ ìš”ì•½(ë‚´ìš© ì¼ë¶€):
{json.dumps(contents_brief, ensure_ascii=False, indent=2)}

í”¼ê²¨ ëª©ë¡(í˜ì´ì§€/ìº¡ì…˜ ì¼ë¶€):
{json.dumps(fig_brief, ensure_ascii=False, indent=2)}
"""

    # 3) í˜¸ì¶œ
    resp = oai.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[
            {"role":"system", "content":system_msg},
            {"role":"user",   "content":user_msg},
        ],
        temperature=0.2,
        response_format={"type":"json_object"},
        max_tokens=1200,
    )
    raw = resp.choices[0].message.content

    # 4) íŒŒì‹± & ë³´ì •
    try:
        layout = json.loads(raw)
        if isinstance(layout, dict) and "items" in layout:
            layout = layout["items"]
        if not isinstance(layout, list):
            raise ValueError("layout is not a list")

        # ìµœì†Œ ê²€ì¦: ì„¹ì…˜ì´ 6ê°œ ëª¨ë‘ ìˆœì„œëŒ€ë¡œ ë“¤ì–´ìˆëŠ”ì§€ ì²´í¬
        keys_in_order = [item.get("key") for item in layout if item.get("type") == "section"]
        if keys_in_order != SECTION_KEYS:
            # ê°•ì œ ë³´ì •: ì„¹ì…˜ì„ ìˆœì„œëŒ€ë¡œ ì¬ë°°ì—´í•˜ê³ , figureë“¤ì€ ì„ì‹œë¡œ ë’¤ì— ë¶™ì„
            raise ValueError("sections order invalid")
        return layout

    except Exception:
        # í´ë°±: ì„¹ì…˜ ì „ë¶€(ìˆœì„œëŒ€ë¡œ) + ëª¨ë“  í”¼ê²¨ë¥¼ ë§¨ ëì—
        fallback = [{"type":"section","key":k} for k in SECTION_KEYS]
        fallback += [{"type":"figure","id":f["id"]} for f in figures]
        return fallback


# 8) Figure detection (vector+raster) & rendering
def is_caption_text(text: str) -> bool:
    first = text.strip().splitlines()[0] if text.strip() else ""
    return bool(CAPTION_RE.search(first))

def get_text_blocks(page: "fitz.Page"):
    blocks = page.get_text("blocks")
    out = []
    for b in blocks:
        x0, y0, x1, y1, text = b[:5]
        btype = b[6] if len(b) > 6 else 0
        out.append({"rect": fitz.Rect(x0,y0,x1,y1), "text": text or "", "type": btype})
    return out

def x_overlap_ratio(a: "fitz.Rect", b: "fitz.Rect") -> float:
    left, right = max(a.x0,b.x0), min(a.x1,b.x1)
    if right <= left: return 0.0
    return (right-left) / max(a.width, b.width)

def rect_union(a: "fitz.Rect", b: "fitz.Rect") -> "fitz.Rect":
    return fitz.Rect(min(a.x0,b.x0), min(a.y0,b.y0), max(a.x1,b.x1), max(a.y1,b.y1))

def expand_rect(r: "fitz.Rect", margin: float, clip: "fitz.Rect") -> "fitz.Rect":
    e = fitz.Rect(r.x0-margin, r.y0-margin, r.x1+margin, r.y1+margin)
    e.x0 = max(e.x0, clip.x0); e.y0 = max(e.y0, clip.y0)
    e.x1 = min(e.x1, clip.x1); e.y1 = min(e.y1, clip.y1)
    return e

def get_visual_rects(page: "fitz.Page"):
    rects = []
    # raster
    for info in page.get_images(full=True):
        xref = info[0]
        for r in page.get_image_rects(xref):
            rects.append(fitz.Rect(r))
    # vector
    try:
        for d in page.get_drawings():
            r = d.get("rect")
            if not r: continue
            rr = fitz.Rect(r)
            if rr.width < 8 or rr.height < 8:
                continue
            rects.append(rr)
    except Exception:
        pass
    return rects

def group_for_caption(caption_rect: "fitz.Rect", rects: list["fitz.Rect"]) -> list["fitz.Rect"]:
    cand = []
    for r in rects:
        above = (r.y1 <= caption_rect.y0) and ((caption_rect.y0 - r.y1) <= MAX_VGAP)
        below = (r.y0 >= caption_rect.y1) and ((r.y0 - caption_rect.y1) <= MAX_VGAP)
        xok   = x_overlap_ratio(r, caption_rect) >= MIN_X_OVERLAP_RATIO
        if xok and (above or below):
            cand.append(r)
    if not cand: return []
    if MERGE_NEAR_IMAGES:
        u = cand[0]
        for r in cand[1:]: u = rect_union(u, r)
        return [u]
    return cand

def render_rect_png_bytes(page: "fitz.Page", rect: "fitz.Rect", scale: float = SCALE) -> bytes:
    mat = fitz.Matrix(scale, scale)
    pix = page.get_pixmap(matrix=mat, clip=rect, alpha=False)
    return pix.tobytes("png")

def extract_and_upload_figures(pdf_path: str) -> list[dict]:
    """
    ë°˜í™˜: [{id, page, caption_text, upload_id, image_block}, ...]
    """
    if fitz is None:
        return []
    out = []
    out_dir = os.path.join(os.path.dirname(pdf_path), "_figures")
    if SAVE_LOCAL_FIGS:
        os.makedirs(out_dir, exist_ok=True)

    doc = fitz.open(pdf_path)
    for pno in range(len(doc)):
        page = doc[pno]
        page_rect = page.rect
        text_blocks = get_text_blocks(page)
        caps = [b for b in text_blocks if b["type"] == 0 and is_caption_text(b["text"])]
        if not caps:
            continue
        vrects = get_visual_rects(page)
        if not vrects:
            continue

        for cidx, cb in enumerate(caps, start=1):
            cap = cb["rect"]
            near = group_for_caption(cap, vrects)
            if not near:
                continue
            uni = cap
            for rr in near:
                uni = rect_union(uni, rr)
            crop = expand_rect(uni, MARGIN, page_rect)
            png_bytes = render_rect_png_bytes(page, crop, SCALE)

            # ì˜µì…˜: ë¡œì»¬ ì €ì¥
            fname = f"p{pno+1:02d}_figure{cidx:02d}.png"
            if SAVE_LOCAL_FIGS:
                with open(os.path.join(out_dir, fname), "wb") as f:
                    f.write(png_bytes)

            # ì—…ë¡œë“œ
            created = create_file_upload(fname, "image/png")
            fid = created.get("id")
            if not fid: 
                continue
            sent = send_file_upload_bytes(fid, png_bytes, fname, "image/png")
            if sent.get("status") != "uploaded":
                continue

            record = {
                "id": f"p{pno+1:02d}_f{cidx:02d}",
                "page": pno+1,
                "caption_text": cb["text"].strip(),
                "upload_id": fid,
                "image_block": image_block_from_upload(fid, caption=""),
            }
            out.append(record)
    doc.close()
    return out


# 9) Build content blocks (with headings + md emphasis)
def build_content_blocks(contents: dict) -> list[dict]:
    """
    ì„¹ì…˜ë³„ë¡œ Heading1 + ë³¸ë¬¸(ë§ˆí¬ë‹¤ìš´ ê°•ì¡° ë°˜ì˜) ë¸”ë¡ì„ ë§Œë“ ë‹¤.
    """
    blocks = []
    for key in SECTION_KEYS:
        body = s(contents.get(key, "")).strip()
        if not body:
            continue
        # Heading1
        blocks.append(_heading(key, 1))
        # ë³¸ë¬¸: MD â†’ rich_text
        rich = md_to_rich_text(body)
        if rich:
            blocks.append(_paragraph_rich(rich))
    return blocks


# 10) Notion page composer (layout ë°˜ì˜)
def create_notion_page_with_layout(meta: dict,
                                   contents: dict,
                                   layout: list[dict],
                                   figures_by_id: dict,
                                   paper_url: str,
                                   pdf_block: dict,
                                   file_hint: str):
    title_prop = ensure_title_prop_name(DATABASE_ID)
    title_val  = meta.get("title") or os.path.splitext(os.path.basename(file_hint))[0]
    props = {
        title_prop: {"title": [{"text": {"content": s(title_val)}}]},
        "Tag": {"multi_select": to_tag_list(meta.get("tag"))},
        "Authors": {"rich_text": [{"text": {"content": s(meta.get("authors"))}}]},
        "Year": {"rich_text": [{"text": {"content": s(meta.get("year"))}}]},
        "Conference/journal": {"rich_text": [{"text": {"content": s(meta.get("conference_journal"))}}]},
        "One sentence": {"rich_text": [{"text": {"content": s(meta.get('one_sentence',''))}}]},
    }

    # 1) ê³µí†µ ìƒë‹¨: URL + PDF
    children = []
    if paper_url: children.append(_bookmark(paper_url))
    if pdf_block: children.append(pdf_block)

        # â¬‡ï¸ Figure 1 ìƒë‹¨ ê³ ì •
    if PROMOTE_FIRST_FIGURE:
        lead_id = pick_first_figure_id(list(figures_by_id.values()))
        # pick_first_figure_idëŠ” dictê°€ ì•„ë‹ˆë¼ list[dict]ë¥¼ ë°›ë„ë¡ í–ˆìœ¼ë‹ˆ ì•½ê°„ ìˆ˜ì •:
        lead_id = pick_first_figure_id([{"id": fid, **rec} for fid, rec in figures_by_id.items()])
        if lead_id and figures_by_id.get(lead_id):
            children.append(figures_by_id[lead_id]["image_block"])

    # 2) ì„ê¸°(ì„¹ì…˜/í”¼ê²¨)
    for item in layout:
        if item.get("type") == "section":
            key = item.get("key")
            body = s(contents.get(key, "")).strip()
            if not body:
                continue
            children.append(_heading(key, 1))
            rich = md_to_rich_text(body)
            if rich:
                children.append(_paragraph_rich(rich))
        elif item.get("type") == "figure":
            fid = item.get("id")
            rec = figures_by_id.get(fid)
            if rec:
                children.append(rec["image_block"])

    # 3) í˜¹ì‹œ ë‚¨ì€ ì„¹ì…˜ì´ ìˆìœ¼ë©´ ë§¨ ë’¤ì— ì¶”ê°€
    used_keys = {i.get("key") for i in layout if i.get("type")=="section"}
    for key in SECTION_KEYS:
        if key not in used_keys and s(contents.get(key,"")).strip():
            children.append(_heading(key, 1))
            children.append(_paragraph_rich(md_to_rich_text(contents[key])))

    return notion.pages.create(parent={"database_id": DATABASE_ID}, properties=props, children=children)


# 11) Pipeline
def process_pdf(path: str):
    print(f"ğŸ“„ Processing: {path}")
    text = read_pdf(path)
    text = strip_unpaired_surrogates(text)
    # (1) ë©”íƒ€
    try:
        meta = call_gpt_meta(text)
    except Exception as e:
        print(f"âš ï¸ GPT(meta) ì˜¤ë¥˜: {e}")
        meta = {"title":"","authors":"","year":"","conference_journal":"","tag":[]}
    if not s(meta.get("year")):
        g = safe_year_guess(os.path.basename(path), text)
        if g: meta["year"] = g

    # (2) contents (MD ê°•ì¡° + ì •í™• í‚¤)
    try:
        contents = call_gpt_contents_marked(text)
    except Exception as e:
        print(f"âš ï¸ GPT(contents) ì˜¤ë¥˜: {e}")
        contents = {k:"" for k in SECTION_KEYS}

    # (3) one-sentence
    try:
        meta["one_sentence"] = s(oai.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[{"role":"system","content":"í•œ ë¬¸ì¥(30ì ì´ë‚´)ë§Œ ë°˜í™˜í•˜ë¼."},
                      {"role":"user","content":f"ë‹¤ìŒ ë…¼ë¬¸ì„ í•œê¸€ë¡œ 30ì ì´ë‚´ í•œ ë¬¸ì¥ ìš”ì•½:\n\"\"\"{text}\"\"\""}],
            temperature=0.2, max_tokens=64,
        ).choices[0].message.content).strip().replace("\n"," ")
    except Exception:
        meta["one_sentence"] = ""

    # (4) paper URL
    try:
        paper_url = find_paper_url(meta.get("title",""), meta.get("authors",""), meta.get("year",""))
    except Exception:
        paper_url = ""

    # (5) PDF ì—…ë¡œë“œ
    pdf_block = None
    try:
        created = create_file_upload(os.path.basename(path), "application/pdf")
        fid = created.get("id")
        sent = send_file_upload_path(fid, path, "application/pdf")
        if sent.get("status") == "uploaded":
            pdf_block = file_block_from_upload(fid)
    except Exception as e:
        print(f"âš ï¸ PDF ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

    # (6) Figures ì¶”ì¶œ/ì—…ë¡œë“œ (ìº¡ì…˜ ë³´ì¡´)
    try:
        figures = extract_and_upload_figures(path)
    except Exception as e:
        print(f"âš ï¸ Figure ì¶”ì¶œ/ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        figures = []

    figures_by_id = {f["id"]: f for f in figures}

    # (7) ë ˆì´ì•„ì›ƒ(ì„¹ì…˜+í”¼ê²¨ ì„ê¸°)
    try:
        layout = call_gpt_layout(contents, figures)
    except Exception as e:
        print(f"âš ï¸ ë ˆì´ì•„ì›ƒ ìƒì„± ì˜¤ë¥˜: {e}")
        layout = [{"type":"section","key":k} for k in SECTION_KEYS] + [{"type":"figure","id":f["id"]} for f in figures]

    # (8) Notion í˜ì´ì§€ ìƒì„±(ë ˆì´ì•„ì›ƒ ë°˜ì˜)
    try:
        res = create_notion_page_with_layout(meta, contents, layout, figures_by_id, paper_url, pdf_block, file_hint=path)
        print(f"âœ… Added to Notion: {res.get('url')}\n")
    except APIResponseError as e:
        print(f"âŒ Notion API ì˜¤ë¥˜: {e}\n")


# 12) Entry
def main():
    pdf_dir = os.path.join(os.getcwd(), "paper")
    if not os.path.isdir(pdf_dir):
        print(f"âŒ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {pdf_dir}")
        sys.exit(1)

    pdfs = sorted(glob.glob(os.path.join(pdf_dir, "*.pdf")))
    if not pdfs:
        print("âŒ paper_pdf í´ë”ì— PDFê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    for p in pdfs:
        process_pdf(p)

if __name__ == "__main__":
    main()

# ingest_papers.py
# ============================================================
# PDF -> GPT(ë©”íƒ€/ë³¸ë¬¸/í•œì¤„ìš”ì•½) -> Notion DB ê¸°ë¡
# + Notion file_uploads RESTë¡œ PDF/ì´ë¯¸ì§€ ì§ì ‘ ì—…ë¡œë“œ
# + ë…¼ë¬¸ URL Bookmark + (ì„ê¸° ë ˆì´ì•„ì›ƒ) Sections & Figures + Contents
# + Notion DB ì†ì„± ìë™ ë§¤í•‘/ë³´ê°• (ì¤‘ë³µ ë§¤í•‘ ê¸ˆì§€, ì‹¤íŒ¨ì‹œ ì•ˆì „ ìƒì„±)
# ============================================================

# 0) Imports
import os, re, glob, json, sys, io, mimetypes
from urllib.parse import quote_plus
import requests
from dotenv import load_dotenv
from pypdf import PdfReader
from notion_client import Client as NotionClient
from notion_client.errors import APIResponseError
from openai import OpenAI

try:
    import fitz  # PyMuPDF
except ImportError:
    fitz = None

# â”€â”€ ì„¤ì •
PROMOTE_FIRST_FIGURE = True
SCALE, MARGIN, MAX_VGAP = 2.0, 6.0, 120.0
MIN_X_OVERLAP_RATIO, MERGE_NEAR_IMAGES, SAVE_LOCAL_FIGS = 0.20, True, False
CAPTION_RE = re.compile(r"^\s*(?:(?:Figure|Fig\.?)\s*[0-9IVX]+|(?:ê·¸ë¦¼|ë„)\s*\d+)\b", re.IGNORECASE)
SECTION_KEYS = [
    "1. ê¸°ì¡´ë¬¸ì œ", "2. ì„ í–‰ì—°êµ¬", "3. ì´ë²ˆ ì—°êµ¬ì˜ ê°œì„ ì ",
    "4. ë¬¸ì œì˜ ì¤‘ìš”ì„±", "5. ì œì•ˆ ì‹œìŠ¤í…œ/ë°©ë²•", "6. ì‹¤í—˜ ê°€ì„¤/ì ˆì°¨",
]

# â”€â”€ ENV/Clients
load_dotenv()
NOTION_TOKEN   = os.getenv("NOTION_TOKEN")
DATABASE_ID    = os.getenv("NOTION_DATABASE_ID")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL   = os.getenv("OPENAI_MODEL", "chatgpt-4o-latest")
USE_GPT_API    = os.getenv("USE_GPT_API", "true").lower() == "true"   # [MODIFIED]
NOTION_VERSION = "2022-06-28"
OPENAI_MAX_TOKENS_META = int(os.getenv("OPENAI_MAX_TOKENS_META", "700"))
OPENAI_MAX_TOKENS_CONTENTS = int(os.getenv("OPENAI_MAX_TOKENS_CONTENTS", "2000"))
OPENAI_MAX_TOKENS_ONESENTENCE = int(os.getenv("OPENAI_MAX_TOKENS_ONESENTENCE", "64"))
OPENAI_MAX_TOKENS_LAYOUT= int(os.getenv("OPENAI_MAX_TOKENS_LAYOUT", "2000"))


_SURROGATE_RE = re.compile(r"[\ud800-\udfff]")



if not (NOTION_TOKEN and DATABASE_ID and OPENAI_API_KEY):
    print("âŒ .envì— NOTION_TOKEN / NOTION_DATABASE_ID / OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    sys.exit(1)

notion = NotionClient(auth=NOTION_TOKEN, notion_version=NOTION_VERSION)
oai    = OpenAI(api_key=OPENAI_API_KEY) if USE_GPT_API else None      # [MODIFIED]

# â”€â”€ DB ìŠ¤í‚¤ë§ˆ
REQUIRED_SCHEMA = {
    "Title":                 {"type": "title"},
    "Tag":                   {"type": "multi_select"},
    "Authors":               {"type": "rich_text"},
    "Year":                  {"type": "rich_text"},
    "Conference/journal":    {"type": "rich_text"},
    "One sentence":          {"type": "rich_text"},
}

ALIASES = {
    "Title": ["title", "name", "paper title", "ë…¼ë¬¸ ì œëª©"],
    "Tag": ["tag", "tags", "í† í”½", "í‚¤ì›Œë“œ"],
    "Authors": ["authors", "author", "ì €ì", "ì €ìëª…"],
    "Year": ["year", "ì—°ë„", "ë°œí‘œì—°ë„"],
    "Conference/journal": ["conference/journal", "venue", "conference", "journal", "í•™íšŒ", "ì €ë„"],
    "One sentence": ["one sentence", "summary", "short summary", "í•œì¤„ìš”ì•½", "ìš”ì•½"],
}


def read_text_file(path: str) -> str:
    try:
        with open(path, "r", encoding="utf-8") as f:
            s = f.read()
        # BOM/ë„ ì œê±° (ìˆì–´ë„ ê¹”ë”í•˜ê²Œ)
        return s.replace("\ufeff", "").replace("\x00", "")
    except FileNotFoundError:
        return ""

_PLACEHOLDER_RE = re.compile(r"\{\{\s*([A-Za-z0-9_]+)\s*\}\}")

def render_prompt(template_str: str, **vars) -> str:
    """
    í…œí”Œë¦¿ ë‚´ {{KEY}} ìë¦¬í‘œì‹œìë¥¼ vars["KEY"] ê°’ìœ¼ë¡œ ì¹˜í™˜.
    ì˜ˆ: render_prompt('Hello {{NAME}}', NAME='world') -> 'Hello world'
    """
    def _sub(m):
        key = m.group(1)
        if key in vars:
            return str(vars[key])
        # ì¹˜í™˜ê°’ ì—†ìœ¼ë©´ ì›ë¬¸ ìœ ì§€(ë””ë²„ê¹… ì›í•˜ë©´ ì—¬ê¸°ì„œ ì—ëŸ¬/ë¡œê·¸ ì²˜ë¦¬)
        return m.group(0)
    return _PLACEHOLDER_RE.sub(_sub, template_str)

def load_prompt_from_files(env_key_filename: str, default_filename: str, base_dir: str | None = None) -> str:
    """
    .envì—ì„œ íŒŒì¼ëª… í‚¤(env_key_filename)ë¡œ íŒŒì¼ëª…ì„ ì½ê³ , base_dir(ê¸°ë³¸ PROMPT_DIR)ê³¼ í•©ì³ ë‚´ìš©ì„ ë¡œë“œ.
    íŒŒì¼ ì—†ìœ¼ë©´ "" ë°˜í™˜.
    """
    if base_dir is None:
        base_dir = os.getenv("PROMPT_DIR", "./prompts")
    filename = os.getenv(env_key_filename, default_filename)
    path = os.path.join(base_dir, filename)
    return read_text_file(path)



def s(v): return "" if v is None else str(v)

def strip_unpaired_surrogates(text: str) -> str:
    if not isinstance(text, str): return text
    text = _SURROGATE_RE.sub("", text)
    try:
        text = text.encode("utf-16", "surrogatepass").decode("utf-16", "ignore")
    except Exception:
        text = text.encode("utf-8", "ignore").decode("utf-8", "ignore")
    return text.replace("\x00", "")

# â”€â”€ DB ë§¤í•‘ (ì¤‘ë³µ ê¸ˆì§€ + ì‹¤íŒ¨ ì‹œ ì•ˆì „ ìƒì„±)
def _norm(s: str) -> str:
    return (s or "").strip().lower()

def _find_title_prop_key(db: dict) -> str | None:
    for k, v in db.get("properties", {}).items():
        if v.get("type") == "title":
            return k
    return None

def _match_by_alias(db: dict, want_name: str, want_type: str, used: set[str]) -> str | None:
    """íƒ€ì…ì´ want_typeì´ê³ , ì´ë¦„ì´ ë³„ì¹­ê³¼ ìœ ì‚¬í•œ ë¯¸ì‚¬ìš© í‚¤ë¥¼ ì°¾ëŠ”ë‹¤."""
    props = db.get("properties", {})
    aliases = [_norm(want_name)] + [_norm(a) for a in ALIASES.get(want_name, [])]
    cands = [k for k, v in props.items() if v.get("type") == want_type and k not in used]

    for k in cands:
        if _norm(k) in aliases:
            return k
    for k in cands:
        nk = _norm(k).replace(" ", "").replace("-", "").replace("_", "")
        for a in aliases:
            na = a.replace(" ", "").replace("-", "").replace("_", "")
            if na and na in nk:
                return k
    return None

def ensure_db_property_map(dbid: str) -> dict:
    """
    1) í˜„ì¬ DB ì¡°íšŒ
    2) ì—†ëŠ” ì†ì„±ë“¤ 'ëª¨ë‘' ëª¨ì•„ì„œ í•œ ë²ˆì˜ updateë¡œ ìƒì„±
    3) ìµœì‹  DBë¥¼ ë‹¤ì‹œ ì¡°íšŒí•´ ìµœì¢… ë§¤í•‘ ë°˜í™˜
    """
    db = notion.databases.retrieve(dbid)
    props = db.get("properties", {})
    used: set[str] = set()
    mapping: dict[str, str] = {}

    # 1) Title(í•„ìˆ˜)
    title_key = _find_title_prop_key(db)
    if not title_key:
        # íƒ€ì´í‹€ì´ ì—†ìœ¼ë©´ ë¨¼ì € íƒ€ì´í‹€ë§Œ ìƒì„±
        notion.databases.update(database_id=dbid, properties={"Title": {"title": {}}})
        db = notion.databases.retrieve(dbid)
        props = db.get("properties", {})
        title_key = _find_title_prop_key(db)
        
    mapping["Title"] = title_key
    used.add(title_key)
    # 2) ë‚˜ë¨¸ì§€ ì›í•˜ëŠ” ì†ì„± ì¤‘ ì´ë¯¸ ìˆëŠ” ê²ƒ ë§¤ì¹­
    missing_props: dict[str, dict] = {}
    for want_name, meta in REQUIRED_SCHEMA.items():
        if want_name == "Title":
            continue
        want_type = meta["type"]

        # íƒ€ì…/ë³„ì¹­ìœ¼ë¡œ ë§¤ì¹­
        found = _match_by_alias(db, want_name, want_type, used)
        if found:
            mapping[want_name] = found
            used.add(found)
        else:
            # ì•„ì§ ì—†ìœ¼ë‹ˆ "í•œêº¼ë²ˆì—" ë§Œë“¤ ëª©ë¡ì— ì¶”ê°€
            missing_props[want_name] = {want_type: {}}

    # 3) ë¶€ì¡±í•œ ì†ì„±ë“¤ í•œ ë²ˆì˜ updateë¡œ ìƒì„± (ìˆì„ ë•Œë§Œ)
    if missing_props:
        notion.databases.update(database_id=dbid, properties=missing_props)
        # ìƒì„± ì§í›„ ìµœì‹  DB ì¬ì¡°íšŒ
        db = notion.databases.retrieve(dbid)
        print(db)
        props = db.get("properties", {})

        # ìƒì„±ëœ ì´ë¦„ ê·¸ëŒ€ë¡œ ë§¤í•‘
        for want_name in missing_props.keys():
            if want_name in props:
                mapping[want_name] = want_name
                used.add(want_name)
            else:
                # ì´ë¦„ ì¶©ëŒ ë“±ìœ¼ë¡œ ìƒì„±ì´ ë‹¤ë¥¸ ì´ë¦„ìœ¼ë¡œ ë˜ì—ˆì„ ê°€ëŠ¥ì„± â†’ íƒ€ì…/ë³„ì¹­ìœ¼ë¡œ ì¬ê²€ìƒ‰
                found = _match_by_alias(db, want_name, REQUIRED_SCHEMA[want_name]["type"], used)
                if found:
                    mapping[want_name] = found
                    used.add(found)
                else:
                    # ìµœí›„ í´ë°±: ì›í•˜ëŠ” ì´ë¦„ìœ¼ë¡œ ê¸°ë¡(ì‹¤íŒ¨ ë¡œê·¸)
                    print(f"âš ï¸ ì†ì„± ìƒì„±/ë§¤ì¹­ ì‹¤íŒ¨: {want_name}")
                    mapping[want_name] = want_name

    return mapping

# â”€â”€ Notion upload helpers
def _notion_headers(extra=None):
    base = {"Authorization": f"Bearer {NOTION_TOKEN}", "Notion-Version": NOTION_VERSION}
    if extra: base.update(extra)
    return base

def create_file_upload(filename: str, content_type: str) -> dict:
    url = "https://api.notion.com/v1/file_uploads"
    payload = {"filename": filename, "content_type": content_type}
    r = requests.post(url, headers=_notion_headers({"Content-Type":"application/json"}), data=json.dumps(payload), timeout=30)
    r.raise_for_status(); return r.json()

def send_file_upload_bytes(file_upload_id: str, data: bytes, filename: str, content_type: str) -> dict:
    url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
    files = {"file": (filename, io.BytesIO(data), content_type)}
    r = requests.post(url, headers=_notion_headers(), files=files, timeout=120)
    r.raise_for_status(); return r.json()

def send_file_upload_path(file_upload_id: str, file_path: str, content_type: str) -> dict:
    url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
    with open(file_path, "rb") as f:
        files = {"file": (os.path.basename(file_path), f, content_type)}
        r = requests.post(url, headers=_notion_headers(), files=files, timeout=120)
    r.raise_for_status(); return r.json()

def file_block_from_upload(file_upload_id: str) -> dict:
    return {"object":"block","type":"file","file":{"type":"file_upload","file_upload":{"id":file_upload_id}}}

def image_block_from_upload(file_upload_id: str, caption: str = "") -> dict:
    block = {"object":"block","type":"image","image":{"type":"file_upload","file_upload":{"id":file_upload_id}}}
    if caption: block["image"]["caption"] = [{"type":"text","text":{"content":caption}}]
    return block

# â”€â”€ ê¸°íƒ€ ìœ í‹¸/í…ìŠ¤íŠ¸/ë§ˆí¬ë‹¤ìš´
def guess_mime(path: str, default="application/octet-stream"):
    mime, _ = mimetypes.guess_type(path); return mime or default

def read_pdf(path: str, max_chars: int = 40000) -> str:
    try:
        reader = PdfReader(path); texts=[]
        for page in reader.pages:
            chunk = page.extract_text() or ""
            if chunk: texts.append(chunk)
            if sum(len(t) for t in texts) >= max_chars: break
        text = "\n\n".join(texts)
        text = re.sub(r"[ \t]+"," ", text); text = re.sub(r"\n{3,}","\n\n", text).strip()
        text = text[:max_chars]
        # â–¶ ë°±ë§¤í„° ì œê±°
        text = trim_trailing_backmatter(text)
        return text
    except Exception as e:
        return f"[PDF READ ERROR] {e}"

def trim_trailing_backmatter(text: str) -> str:
    """
    1) 'References/Bibliography/Appendix/Acknowledgments' ê°™ì€ í—¤ë”©ì„ ì°¾ì•„ ê·¸ ì§€ì ë¶€í„° ì˜ë¼ëƒ„.
    2) ëª» ì°¾ìœ¼ë©´ í•˜ë‹¨ë¶€ì—ì„œ ì°¸ê³ ë¬¸í—ŒìŠ¤ëŸ¬ìš´ ë¼ì¸ ë¹„ìœ¨ì„ ë³´ê³  ì»·ì˜¤í”„.
    3) ê³¼ë„ íŠ¸ë¦¼ ë°©ì§€: ì „ì²´ì˜ 60% ì´ì „ì—ì„œ ìë¥´ì§€ ì•ŠìŒ.
    ENV:
      TRIM_BACKMATTER (true/false), BACKMATTER_MIN_POS_RATIO (0~1), BACKMATTER_TAIL_LINES (int)
    """
    if not text:
        return text

    enabled = os.getenv("TRIM_BACKMATTER", "true").lower() == "true"
    if not enabled:
        return text

    import regex as re  # ë” ê°•í•œ ì •ê·œì‹ ì—”ì§„ì´ ìˆìœ¼ë©´ ì¢‹ì§€ë§Œ, ì—†ìœ¼ë©´ reë¡œ ë°”ê¿”ë„ OK
    MIN_POS_RATIO = float(os.getenv("BACKMATTER_MIN_POS_RATIO", "0.60"))  # ë³¸ë¬¸ 60% ì´í›„ë§Œ ì»· í—ˆìš©
    TAIL_LINES = int(os.getenv("BACKMATTER_TAIL_LINES", "400"))           # ì•„ë˜ìª½ ìŠ¤ìº” ë¼ì¸ ìˆ˜
    body_len = len(text)
    min_pos = int(body_len * MIN_POS_RATIO)

    # 1) ëª…ì‹œì  í—¤ë”© ë§¤ì¹˜ (ë©€í‹°ë¼ì¸)
    heading_re = re.compile(
        r"(?m)^\s*(references|bibliography|appendix|appendices|acknowledg?e?ments)\s*$",
        re.IGNORECASE
    )
    m = heading_re.search(text)
    if m and m.start() >= min_pos:
        return text[:m.start()].rstrip()

    # 2) ì•„ë˜ìª½ì—ì„œ ìœ„ë¡œ ì°¸ê³ ë¬¸í—Œ ë¼ì¸ ë¹„ìœ¨ ì²´í¬
    #    íŒ¨í„´ ì˜ˆ: [12] Foo..., 12. Bar..., (2019), 2019., doi:, arXiv:, Proc. of ...
    ref_like_bul = re.compile(
        r"^\s*(\[\d{1,3}\]|\d{1,3}\.|â€¢|-)\s+.+", re.IGNORECASE
    )
    year_like = re.compile(r"\b(19[6-9]\d|20[0-4]\d)\b")
    doi_like  = re.compile(r"\b(doi:|https?://(dx\.)?doi\.org/|arxiv:)\b", re.IGNORECASE)
    proc_like = re.compile(r"\b(proceedings|proc\.|vol\.|no\.|pp\.)\b", re.IGNORECASE)

    lines = text.splitlines()
    n = len(lines)
    start_line = max(0, n - TAIL_LINES)
    tail = lines[start_line:]

    def is_ref_line(line: str) -> bool:
        l = line.strip()
        if not l:
            return False
        hits = 0
        if ref_like_bul.match(l): hits += 1
        if year_like.search(l):   hits += 1
        if doi_like.search(l):    hits += 1
        if proc_like.search(l):   hits += 1
        # ê¸°ì¤€: ìœ„ íŠ¹ì§• ì¤‘ 2ê°œ ì´ìƒ
        return hits >= 2

    # ì•„ë˜ìª½ì—ì„œ ìœ„ë¡œ ê°€ë©´ì„œ "ì—°ì†ì ìœ¼ë¡œ ì°¸ê³ ë¬¸í—Œ ê°™ì€" êµ¬ê°„ì˜ ì‹œì‘ ì§€ì  ì°¾ê¸°
    ref_run = 0
    run_needed = 6  # ìµœì†Œ 6ì¤„ ì •ë„ ì—°ì†ìœ¼ë¡œ ì°¸ê³ ë¬¸í—Œ ëŠë‚Œì´ë©´ ì»·
    cutoff_idx_in_tail = None

    for i in range(len(tail) - 1, -1, -1):
        if is_ref_line(tail[i]):
            ref_run += 1
        else:
            if ref_run >= run_needed:
                cutoff_idx_in_tail = i + 1
                break
            ref_run = 0

    # ëê¹Œì§€ ì™”ì„ ë•Œë„ ê¸´ ëŸ¬ë‹ì´ ìœ ì§€ë˜ë©´ tail ì‹œì‘ì—ì„œ ì»·
    if cutoff_idx_in_tail is None and ref_run >= run_needed:
        cutoff_idx_in_tail = 0

    if cutoff_idx_in_tail is not None:
        abs_cut = start_line + cutoff_idx_in_tail
        # ê³¼ë„ íŠ¸ë¦¼ ë°©ì§€
        cut_pos = sum(len(l) + 1 for l in lines[:abs_cut])
        if cut_pos >= min_pos:
            return "\n".join(lines[:abs_cut]).rstrip()

    return text

def safe_year_guess(filename: str, text: str) -> str:
    cand = re.findall(r"\b(19[7-9]\d|20[0-4]\d|2025|2026)\b", filename + " " + text)
    return cand[0] if cand else ""

MD_PATTERNS = [
    (re.compile(r"\*\*(.+?)\*\*"), {"bold": True}),
    (re.compile(r"__(.+?)__"),     {"underline": True}),
    (re.compile(r"\*(.+?)\*"),     {"italic": True}),
]
def md_to_rich_text(md: str) -> list[dict]:
    spans=[(0, md, {})]
    for pat, anno in MD_PATTERNS:
        new=[]
        for _, text, attrs in spans:
            pos=0
            for m in pat.finditer(text):
                if m.start()>pos: new.append((0, text[pos:m.start()], attrs))
                new.append((0, m.group(1), {**attrs, **anno})); pos=m.end()
            if pos<len(text): new.append((0, text[pos:], attrs))
        spans=new
    rich=[]
    for _, piece, attrs in spans:
        if not piece: continue
        rich.append({"type":"text","text":{"content":piece},"annotations":{
            "bold":attrs.get("bold",False),"italic":attrs.get("italic",False),
            "underline":attrs.get("underline",False),"code":False,"strikethrough":False,"color":"default"}})
    return rich

# â”€â”€ ì™¸ë¶€ URL ì°¾ê¸°
def find_paper_url(title: str, authors: str = "", year: str = "") -> str:
    if not title: return ""
    title_q = quote_plus(title.strip())
    try:
        url = f"https://api.crossref.org/works?query.title={title_q}&rows=3"
        r = requests.get(url, timeout=10)
        if r.ok:
            items = r.json().get("message", {}).get("items", [])
            title_norm = re.sub(r"\s+"," ",title).strip().lower()
            best=None
            for it in items:
                it_title=" ".join(it.get("title", [])).strip()
                it_norm=re.sub(r"\s+"," ",it_title).strip().lower()
                score = 2 if it_norm==title_norm else (1 if title_norm in it_norm or it_norm in title_norm else 0)
                if year and str(it.get("issued",{}).get("date-parts",[[None]])[0][0])==year: score+=1
                if not best or score>best[0]: best=(score,it)
            if best and best[0]>=1:
                it=best[1]; doi=it.get("DOI"); link=it.get("URL")
                if doi: return f"https://doi.org/{doi}"
                if link: return link
    except Exception: pass
    try:
        r = requests.get(f"http://export.arxiv.org/api/query?search_query=ti:{title_q}", timeout=10)
        if r.ok and "<entry>" in r.text:
            m=re.search(r"<id>(https?://arxiv\.org/abs/[^\<]+)</id>", r.text)
            if m: return m.group(1).strip()
    except Exception: pass
    try:
        s2=f"https://api.semanticscholar.org/graph/v1/paper/search?query={title_q}&limit=1&fields=url,externalIds,title,year"
        r=requests.get(s2, timeout=10)
        if r.ok and r.json().get("data"):
            d0=r.json()["data"][0]
            if d0.get("url"): return d0["url"]
            ext=d0.get("externalIds",{}); 
            if "DOI" in ext: return f"https://doi.org/{ext['DOI']}"
    except Exception: pass
    return ""

def call_gpt_meta(text_snippet: str) -> dict:
    if not USE_GPT_API:
        print("[META] USE_GPT_API=False â†’ í…ŒìŠ¤íŠ¸ê°’ ë°˜í™˜")
        return {
            "title": "Test Paper Title",
            "authors": "Doe, J.; Smith, A.",
            "year": "2023",
            "conference_journal": "CHI Conference",
            "tag": ["HCI", "haptics", "test"],
        }

    # 0) ì…ë ¥ ì „ì²˜ë¦¬/ë¡œê·¸
    text_snippet = strip_unpaired_surrogates(text_snippet)
    #print("[META] snippet_head:", (text_snippet[:300] + " â€¦") if len(text_snippet) > 300 else text_snippet)

    # 1) í”„ë¡¬í”„íŠ¸ íŒŒì¼ ê²½ë¡œ/ë‚´ìš© ë¡œë“œ
    base_dir  = os.getenv("PROMPT_DIR", "./prompts")
    sys_path  = os.path.join(base_dir, os.getenv("PROMPT_META_SYSTEM", "meta.system.txt"))
    user_path = os.path.join(base_dir, os.getenv("PROMPT_META_USER",   "meta.user.txt"))
    #print("[META] system_path:", sys_path)
    #print("[META] user_path  :", user_path)

    system_msg = read_text_file(sys_path)
    user_tpl   = read_text_file(user_path)

    if not system_msg:
        system_msg = "ë„ˆëŠ” ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. JSONë§Œ ë°˜í™˜í•˜ë¼."
        print("[META][WARN] system prompt íŒŒì¼ ë¹„ì–´ìˆìŒ â†’ ê¸°ë³¸ ë¬¸êµ¬ ì‚¬ìš©")
    if not user_tpl:
        user_tpl = (
            "ë‹¤ìŒ ìŠ¤ë‹ˆí«ìœ¼ë¡œ title, authors, year, conference_journal, tagë§Œ í¬í•¨í•œ JSONì„ ë§Œë“¤ì–´ë¼.\n"
            "\"\"\"{{TEXT_SNIPPET}}\"\"\""
        )
        print("[META][WARN] user prompt íŒŒì¼ ë¹„ì–´ìˆìŒ â†’ ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©")

    user_msg = render_prompt(user_tpl, TEXT_SNIPPET=text_snippet)
    #print("[META] system_msg_head:", (system_msg[:200] + " â€¦") if len(system_msg) > 200 else system_msg)
    #print("[META] user_msg_head  :", (user_msg[:200] + " â€¦") if len(user_msg) > 200 else user_msg)

    # 2) í˜¸ì¶œ
    resp = oai.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user",   "content": user_msg},
        ],
        temperature=0.2,
        response_format={"type": "json_object"},
        max_tokens=OPENAI_MAX_TOKENS_META,
    )
    raw = resp.choices[0].message.content or "{}"
    #print("[META] raw_resp:", raw)

    # 3) íŒŒì‹±(ë˜í•‘/ëŒ€ì†Œë¬¸ì/ë™ì˜ì–´ ë³´ê°•)
    try:
        data = json.loads(raw)
    except Exception as e:
        print("[META][ERROR] JSON íŒŒì‹± ì‹¤íŒ¨:", e)
        return {"title":"", "authors":"", "year":"", "conference_journal":"", "tag":[], "_raw": raw}

    # ì‘ë‹µì´ {"meta": {...}} ì´ê±°ë‚˜ ë°”ë¡œ {...} ì¼ ìˆ˜ ìˆìŒ
    payload = data.get("meta") if isinstance(data, dict) else None
    if not isinstance(payload, dict):
        payload = data if isinstance(data, dict) else {}

    # í‚¤ ì •ê·œí™”/ë™ì˜ì–´ ë§µí•‘
    def _norm(s): return (s or "").strip().lower().replace(" ", "").replace("-", "_")
    norm_map = { _norm(k): k for k in ["title","authors","year","conference_journal","tag"] }

    # ë™ì˜ì–´ â†’ í‘œì¤€í‚¤
    syn = {
        "conference": "conference_journal",
        "venue": "conference_journal",
        "journal": "conference_journal",
        "conf": "conference_journal",
    }

    out = {"title":"", "authors":"", "year":"", "conference_journal":"", "tag":[]}

    for k, v in payload.items():
        nk = _norm(k)
        if nk in norm_map:
            key = norm_map[nk]
        elif nk in syn:
            key = syn[nk]
        else:
            continue  # ì•Œ ìˆ˜ ì—†ëŠ” í‚¤ëŠ” ë¬´ì‹œ

        # ê°’ í›„ì²˜ë¦¬
        if key == "tag":
            if isinstance(v, (list, tuple, set)):
                out["tag"] = [str(x).strip() for x in v if str(x).strip()]
            elif isinstance(v, str):
                out["tag"] = [x.strip() for x in v.split(",") if x.strip()]
            else:
                out["tag"] = []
        elif key == "authors":
            if isinstance(v, (list, tuple)):
                out["authors"] = "; ".join([str(x).strip() for x in v if str(x).strip()])
            else:
                out["authors"] = str(v).strip()
        else:
            out[key] = str(v).strip()

    # 4) í•„ë“œë³„ ìƒíƒœ ë¡œê·¸
    # print("[META] parsed =>",
    #       "title=", bool(out["title"]), 
    #       "authors=", bool(out["authors"]),
    #       "year=", out["year"],
    #       "conf_journal=", bool(out["conference_journal"]),
    #       "tag_len=", len(out["tag"]))

    # 5) ë¹„ì–´ìˆëŠ” í•„ë“œ ë³´ì¡° ì¶”ì¶œ(ì•½ì‹)
    #  - PDF metadataì—ì„œë„ ì¢…ì¢… ì œëª©/ì €ì ë‚˜ì˜´
    #  - íŒŒì¼ëª…/ë³¸ë¬¸ì—ì„œ ì—°ë„ ì¶”ì •ì€ ì´ë¯¸ elsewhereì—ì„œ ìˆ˜í–‰
    if (not out["title"] or not out["authors"]) and 'PdfReader' in globals():
        # (ì„ íƒ) í•„ìš” ì‹œ ì—¬ê¸°ì— reader.metadata ì¶”ì¶œ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
        pass

    # 6) ìµœì¢… ë°˜í™˜
    return out





def call_gpt_contents_marked(text_snippet: str) -> dict:
    if not USE_GPT_API:
        return {k: f"{k} : í…ŒìŠ¤íŠ¸ìš© ì„ì˜ ë‚´ìš©ì…ë‹ˆë‹¤." for k in SECTION_KEYS}

    text_snippet = strip_unpaired_surrogates(text_snippet)

    base_dir  = os.getenv("PROMPT_DIR", "./prompts")
    sys_path  = os.path.join(base_dir, os.getenv("PROMPT_CONTENTS_SYSTEM", "contents.system.txt"))
    user_path = os.path.join(base_dir, os.getenv("PROMPT_CONTENTS_USER",   "contents.user.txt"))

    #print(sys_path)
    

    system_msg = read_text_file(sys_path)
    user_tpl   = read_text_file(user_path)

    # ğŸ” ì—¬ê¸°ì„œ ì‹¤ì œ ë…¼ë¬¸ ë³¸ë¬¸ì„ {{TEXT_SNIPPET}}ì— ê½‚ì•„ ë„£ìŒ
    user_msg = render_prompt(user_tpl, TEXT_SNIPPET=text_snippet)
    #print("[CONTENTS user_msg]\\n", user_msg[:600])

    # (ì˜µì…˜) ì¹˜í™˜ì´ ì•ˆ ëœ ìë¦¬í‘œì‹œì ë‚¨ì•„ìˆìœ¼ë©´ ê²½ê³ /ì˜ˆì™¸ë¡œ ì¡ì•„ë‚´ë„ ì¢‹ìŒ
    # if "{{TEXT_SNIPPET}}" in user_msg: raise ValueError("TEXT_SNIPPET ì¹˜í™˜ ì‹¤íŒ¨: í…œí”Œë¦¿/í‚¤ í™•ì¸")

    # í´ë°±
    if not system_msg:
        system_msg = "ë„ˆëŠ” ë…¼ë¬¸ ìš”ì•½ì— íŠ¹í™”ëœ í•œêµ­ì–´ ì—°êµ¬ ë¹„ì„œë‹¤. ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•´ë¼."
    if not user_msg:
        user_msg = 'ë‹¤ìŒ ìŠ¤ë‹ˆí«ìœ¼ë¡œ "contents" ë”•ì…”ë„ˆë¦¬ë¥¼ ë§Œë“¤ì–´ë¼:\n"""' + text_snippet + '"""'

    resp = oai.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user",   "content": user_msg},
        ],
        temperature=0.2,
        response_format={"type": "json_object"},
        max_tokens=OPENAI_MAX_TOKENS_CONTENTS,
    )
    raw = resp.choices[0].message.content
    #print("[CONTENTS raw]\\n", raw)
    data = json.loads(raw)
    
    contents = data.get("contents", data)
    #print("[CONTENTS keys]", list(contents.keys()))
    for k in SECTION_KEYS:
        contents.setdefault(k, "")
    return contents



def call_gpt_one_sentence(text_snippet: str) -> str:
    if not USE_GPT_API:
        return "í…ŒìŠ¤íŠ¸ìš© í•œì¤„ ìš”ì•½ì…ë‹ˆë‹¤."

    text_snippet = strip_unpaired_surrogates(text_snippet)

    base_dir  = os.getenv("PROMPT_DIR", "./prompts")
    sys_path  = os.path.join(base_dir, os.getenv("PROMPT_ONELINE_SYSTEM", "oneline.system.txt"))
    user_path = os.path.join(base_dir, os.getenv("PROMPT_ONELINE_USER",   "oneline.user.txt"))

    system_msg = read_text_file(sys_path) or "í•œ ë¬¸ì¥(30ì ì´ë‚´)ë§Œ ë°˜í™˜í•˜ë¼."
    user_tpl   = read_text_file(user_path) or 'ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ 30ì ì´ë‚´ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½:\n"""{{TEXT_SNIPPET}}"""'

    user_msg = render_prompt(user_tpl, TEXT_SNIPPET=text_snippet)
    resp = oai.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user",   "content": user_msg},
        ],
        temperature=0.2,
        max_tokens=OPENAI_MAX_TOKENS_ONESENTENCE,
    )
    return (resp.choices[0].message.content or "").strip().replace("\n", " ")


def call_gpt_layout(contents: dict, figures: list[dict]) -> list[dict]:
    """
    ëª©í‘œ:
    - ì„¹ì…˜ ìˆœì„œ(1â†’6) ê³ ì •
    - ëª¨ë“  figureë¥¼ ì •í™•íˆ 1íšŒì”© 'ê´€ë ¨ ì„¹ì…˜ ë°”ë¡œ ë’¤'ì— ë°°ì¹˜ (GPTê°€ ë¹¼ë¨¹ì–´ë„ ì½”ë“œê°€ ê°•ì œ ë³´ì •)
    - ìº¡ì…˜/ì„¹ì…˜ ìš”ì•½ì€ ê¸°ë³¸ì ìœ¼ë¡œ 'ì „ì²´' ì‚¬ìš© (ENVë¡œ ì œí•œ ê°€ëŠ¥)
    - í’ë¶€í•œ ë””ë²„ê·¸ ë¡œê·¸ ì¶œë ¥
    """

    # ====== 0) ENVë¡œ ê¸¸ì´ ì œì–´ (ê¸°ë³¸: ì „ì²´ ì‚¬ìš©) ======
    try:
        MAX_SECTION_CHARS = int(os.getenv("PROMPT_SECTION_CHARS", "0"))  # 0ì´ë©´ ì „ì²´
        MAX_CAPTION_CHARS = int(os.getenv("PROMPT_CAPTION_CHARS", "0"))  # 0ì´ë©´ ì „ì²´
    except Exception:
        MAX_SECTION_CHARS = 0
        MAX_CAPTION_CHARS = 0

    def _brief_full(s: str, n: int = 0) -> str:
        s = strip_unpaired_surrogates(s or "")
        return s if n <= 0 else s[:n]

    # ====== 1) ì…ë ¥ ì •ë¦¬ (ì„¹ì…˜/í”¼ê²¨ ë³¸ë¬¸ ê·¸ëŒ€ë¡œ íˆ¬ì…) ======
    contents_brief = {k: _brief_full(contents.get(k, ""), MAX_SECTION_CHARS) for k in SECTION_KEYS}
    fig_brief = [{
        "id": f.get("id"),
        "page": int(f.get("page", 0) or 0),
        # â–¶ ìº¡ì…˜ ì „ì²´ (ìë¥´ì§€ ì•ŠìŒ; í•„ìš”ì‹œ ENVë¡œ ì œí•œ)
        "caption": _brief_full(f.get("caption_text", ""), MAX_CAPTION_CHARS)
    } for f in figures]

    # ====== 2) ì…ë ¥ ë¡œê·¸ ======
    #print("\n[LAYOUT] === SECTION BRIEF (ìš”ì•½/ì „ì²´) ===")
    for k in SECTION_KEYS:
        txt = contents_brief.get(k, "")
        #print(f"\n--- {k} ---\n{txt[:600]}{'...' if len(txt)>600 else ''}")

    #print("\n[LAYOUT] === FIGURE BRIEF (ìº¡ì…˜) ===")
    for fb in fig_brief:
        cap = fb.get("caption","")
        #print(f"\n- {fb['id']}: {cap[:600]}{'...' if len(cap)>600 else ''}")

    # ====== 3) GPT í˜¸ì¶œ ======
    base_dir  = os.getenv("PROMPT_DIR", "./prompts")
    sys_path  = os.path.join(base_dir, os.getenv("PROMPT_LAYOUT_SYSTEM", "layout.system.txt"))
    user_path = os.path.join(base_dir, os.getenv("PROMPT_LAYOUT_USER",   "layout.user.txt"))

    system_msg = read_text_file(sys_path) or "ë„ˆëŠ” Notion í˜ì´ì§€ ë ˆì´ì•„ì›ƒì„ ì„¤ê³„í•˜ëŠ” ì‹œìŠ¤í…œì´ë‹¤. ë°˜ë“œì‹œ JSON ê°ì²´ë§Œ ë°˜í™˜í•˜ë¼."
    user_tpl   = read_text_file(user_path) or (
        # SECTION_KEYSë„ í…œí”Œë¦¿ì— ë„£ì–´ì£¼ê¸°
        "í•„ìˆ˜ ê·œì¹™:\n1) 6ê°œ ì„¹ì…˜ì„ ì´ ìˆœì„œë¡œ ëª¨ë‘ í¬í•¨: {{SECTION_KEYS}}\n"
        "2) ëª¨ë“  í”¼ê²¨ idë¥¼ ì •í™•íˆ í•œ ë²ˆì”© í¬í•¨\n\n"
        "ì„¹ì…˜ ìš”ì•½:\n{{CONTENTS_BRIEF_JSON}}\n\ní”¼ê²¨ ëª©ë¡:\n{{FIG_BRIEF_JSON}}"
    )

    user_msg = render_prompt(
        user_tpl,
        SECTION_KEYS=json.dumps(SECTION_KEYS, ensure_ascii=False),
        CONTENTS_BRIEF_JSON=json.dumps(contents_brief, ensure_ascii=False, indent=2),
        FIG_BRIEF_JSON=json.dumps(fig_brief, ensure_ascii=False, indent=2),
    )

    items_from_gpt = []
    if USE_GPT_API:
        try:
            resp = oai.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[{"role":"system","content":system_msg},{"role":"user","content":user_msg}],
                temperature=0.2,
                response_format={"type":"json_object"},
                max_tokens=OPENAI_MAX_TOKENS_LAYOUT,
            )
            raw = resp.choices[0].message.content or "{}"
            data = json.loads(raw)
            items_from_gpt = data.get("items", []) if isinstance(data, dict) else []
        except Exception as e:
            print(f"[LAYOUT][ERROR] GPT í˜¸ì¶œ/íŒŒì‹± ì‹¤íŒ¨: {e}")
    else:
        print("[LAYOUT] USE_GPT_API=False â†’ ì„ì‹œ ë¹ˆ ë ˆì´ì•„ì›ƒ ì‚¬ìš©")

    # ====== 4) ì •í•©ì„±/í´ë¦°ì—… (GPT ê²°ê³¼ë¥¼ ìµœëŒ€í•œ ì‚´ë¦¬ë˜, ëˆ„ë½ figureëŠ” ì±„ì›€) ======
    valid_ids = {f["id"] for f in figures}
    # figure â†’ ì„¹ì…˜ ë²„í‚·
    buckets: dict[str, list[str]] = {k: [] for k in SECTION_KEYS}
    used_figs: set[str] = set()

    # (A) GPTê°€ ì¤€ itemsë¥¼ í›‘ì–´ì„œ, ì„¹ì…˜ ë°”ë¡œ ë’¤ì— ì˜¨ figureë§Œ í•´ë‹¹ ì„¹ì…˜ ë²„í‚·ì— ë‹´ê¸°
    last_section = None
    for it in items_from_gpt:
        t = it.get("type")
        if t == "section" and it.get("key") in SECTION_KEYS:
            last_section = it["key"]
        elif t == "figure":
            fid = it.get("id")
            if fid in valid_ids and fid not in used_figs and last_section in buckets:
                buckets[last_section].append(fid)
                used_figs.add(fid)
            else:
                # ë¬´íš¨/ì¤‘ë³µ/ì„¹ì…˜ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ â†’ ì¼ë‹¨ íŒ¨ìŠ¤(ì•„ë˜ì—ì„œ ë³´ì •)
                pass

    # (B) ëˆ„ë½ figureë“¤ ì°¾ì•„ì„œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì„¹ì…˜ ë°°ì • (ë¬´ì¡°ê±´ ì „ë¶€ í¬í•¨ë˜ë„ë¡)
    def _heuristic_section(cap: str) -> str:
        c = (cap or "").lower()
        # ì‹¤í—˜/ì¸¡ì •/ê²°ê³¼/í‰ê°€ â†’ ì„¹ì…˜ 6
        if any(w in c for w in ["experiment", "user study", "measurement", "measured",
                                "result", "evaluation", "participants", "task", "procedure", "accuracy", "comparison"]):
            return "6. ì‹¤í—˜ ê°€ì„¤/ì ˆì°¨"
        # ë°©ë²•/ì‹œìŠ¤í…œ/ì•„í‚¤í…ì²˜ â†’ ì„¹ì…˜ 5
        if any(w in c for w in ["method", "system", "architecture", "pipeline",
                                "approach", "algorithm", "implementation", "design", "controller", "circuit"]):
            return "5. ì œì•ˆ ì‹œìŠ¤í…œ/ë°©ë²•"
        # ì„ í–‰/ë°°ê²½ â†’ ì„¹ì…˜ 2
        if any(w in c for w in ["related", "prior", "previous", "background", "state-of-the-art"]):
            return "2. ì„ í–‰ì—°êµ¬"
        # ê°œì„ ì /ì¤‘ìš”ì„± í‚¤ì›Œë“œ
        if any(w in c for w in ["improvement", "advantage", "novelty", "contribution"]):
            return "3. ì´ë²ˆ ì—°êµ¬ì˜ ê°œì„ ì "
        if any(w in c for w in ["motivation", "importance", "significance"]):
            return "4. ë¬¸ì œì˜ ì¤‘ìš”ì„±"
        # í´ë°±: 5 (ë°©ë²•)
        return "5. ì œì•ˆ ì‹œìŠ¤í…œ/ë°©ë²•"

    missing_ids = [fid for fid in valid_ids if fid not in used_figs]
    if missing_ids:
        print(f"[LAYOUT][WARN] GPTê°€ ëˆ„ë½í•œ figure ìˆ˜: {len(missing_ids)} â†’ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì±„ì›€")
    # ìº¡ì…˜ ë§µ
    cap_by_id = {fb["id"]: fb.get("caption","") for fb in fig_brief}
    for fid in missing_ids:
        sec = _heuristic_section(cap_by_id.get(fid, ""))
        buckets[sec].append(fid)
        used_figs.add(fid)
        # ë¡œê·¸: ì–´ë–¤ ì„¹ì…˜ìœ¼ë¡œ ë³´ì •ëëŠ”ì§€
        cap = cap_by_id.get(fid, "")
        print(f"[LAYOUT][FILL] figure={fid} â†’ section='{sec}' by heuristic")
        cap_snippet = cap[:200].replace("\n", " ")
        print(f"  â€¢ caption: {cap_snippet}{'...' if len(cap) > 200 else ''}")


    # ====== 5) ìµœì¢… items: 1â†’6 ì„¹ì…˜ì„ ê³ ì • ìˆœì„œë¡œ ê¹”ê³ , ê° ì„¹ì…˜ ë’¤ì— ë²„í‚· figureë¥¼ ì—°ë‹¬ì•„ ì‚½ì… ======
    final_items: list[dict] = []
    for sec in SECTION_KEYS:
        final_items.append({"type":"section","key":sec})
        for fid in buckets[sec]:
            final_items.append({"type":"figure","id":fid})

    # ====== 6) ìµœì¢… ì‹œí€€ìŠ¤ ë¡œê·¸ + ì»¤ë²„ë¦¬ì§€ ê²€ì¦ ======
    seq = ["{sec:"+it["key"]+"}" if it["type"]=="section" else "{fig:"+it["id"]+"}" for it in final_items]
    print("\n[LAYOUT] === FINAL ITEMS ORDER ===")
    print(" ".join(seq))

    placed_figs = [it["id"] for it in final_items if it["type"]=="figure"]
    if set(placed_figs) != valid_ids or len(placed_figs) != len(valid_ids):
        print("[LAYOUT][ERROR] figure ì»¤ë²„ë¦¬ì§€ê°€ 100%ê°€ ì•„ë‹˜! (ë…¼ë¦¬ ì ê²€ í•„ìš”)")
        print("  placed:", placed_figs)
        print("  valid :", sorted(valid_ids))

    return final_items



# â”€â”€ Figure ì¶”ì¶œ
def get_text_blocks(page: "fitz.Page"):
    blocks = page.get_text("blocks"); out=[]
    for b in blocks:
        x0,y0,x1,y1,text = b[:5]; btype = b[6] if len(b)>6 else 0
        out.append({"rect":fitz.Rect(x0,y0,x1,y1),"text":text or "","type":btype})
    return out
def is_caption_text(text: str) -> bool:
    first = text.strip().splitlines()[0] if text.strip() else ""
    return bool(CAPTION_RE.search(first))
def x_overlap_ratio(a, b):
    left, right = max(a.x0,b.x0), min(a.x1,b.x1)
    if right<=left: return 0.0
    return (right-left)/max(a.width,b.width)
def rect_union(a,b): return fitz.Rect(min(a.x0,b.x0),min(a.y0,b.y0),max(a.x1,b.x1),max(a.y1,b.y1))
def expand_rect(r, margin, clip):
    e = fitz.Rect(r.x0-margin, r.y0-margin, r.x1+margin, r.y1+margin)
    e.x0=max(e.x0,clip.x0); e.y0=max(e.y0,clip.y0); e.x1=min(e.x1,clip.x1); e.y1=min(e.y1,clip.y1); return e
def get_visual_rects(page):
    rects=[]
    for info in page.get_images(full=True):
        xref=info[0]
        for r in page.get_image_rects(xref): rects.append(fitz.Rect(r))
    try:
        for d in page.get_drawings():
            r=d.get("rect")
            if not r: continue
            rr=fitz.Rect(r)
            if rr.width<8 or rr.height<8: continue
            rects.append(rr)
    except Exception: pass
    return rects
def group_for_caption(caption_rect, rects):
    cand=[]
    for r in rects:
        above=(r.y1<=caption_rect.y0) and ((caption_rect.y0-r.y1)<=MAX_VGAP)
        below=(r.y0>=caption_rect.y1) and ((r.y0-caption_rect.y1)<=MAX_VGAP)
        xok = x_overlap_ratio(r, caption_rect) >= MIN_X_OVERLAP_RATIO
        if xok and (above or below): cand.append(r)
    if not cand: return []
    if MERGE_NEAR_IMAGES:
        u=cand[0]
        for r in cand[1:]: u=rect_union(u,r)
        return [u]
    return cand
def render_rect_png_bytes(page, rect, scale=SCALE):
    mat=fitz.Matrix(scale,scale); pix=page.get_pixmap(matrix=mat, clip=rect, alpha=False)
    return pix.tobytes("png")

def extract_and_upload_figures(pdf_path: str) -> list[dict]:
    if fitz is None: return []
    out=[]; out_dir=os.path.join(os.path.dirname(pdf_path), "_figures")
    if SAVE_LOCAL_FIGS: os.makedirs(out_dir, exist_ok=True)
    doc=fitz.open(pdf_path)
    for pno in range(len(doc)):
        page=doc[pno]; page_rect=page.rect
        caps=[b for b in get_text_blocks(page) if b["type"]==0 and is_caption_text(b["text"])]
        if not caps: continue
        vrects=get_visual_rects(page)
        if not vrects: continue
        for cidx, cb in enumerate(caps, start=1):
            cap=cb["rect"]; near=group_for_caption(cap, vrects)
            if not near: continue
            uni=cap
            for rr in near: uni=rect_union(uni, rr)
            crop=expand_rect(uni, MARGIN, page_rect)
            png_bytes=render_rect_png_bytes(page, crop, SCALE)
            fname=f"p{pno+1:02d}_figure{cidx:02d}.png"
            if SAVE_LOCAL_FIGS:
                with open(os.path.join(out_dir, fname),"wb") as f: f.write(png_bytes)
            created=create_file_upload(fname, "image/png"); fid=created.get("id")
            if not fid: continue
            sent=send_file_upload_bytes(fid, png_bytes, fname, "image/png")
            if sent.get("status")!="uploaded": continue
            out.append({
                "id": f"p{pno+1:02d}_f{cidx:02d}",
                "page": pno+1,
                "caption_text": cb["text"].strip(),
                "upload_id": fid,
                "image_block": image_block_from_upload(fid, caption=""),
            })
    doc.close(); return out

# â”€â”€ í˜ì´ì§€ ìƒì„±
def pick_first_figure_id(figures: list[dict]) -> str | None:
    if not figures: return None
    return sorted(figures, key=lambda f: (int(f.get("page", 10**9) or 10**9), str(f.get("id",""))))[0].get("id")

def create_notion_page_with_layout(meta, contents, layout, figures_by_id, paper_url, pdf_block, file_hint):
    prop_map = ensure_db_property_map(DATABASE_ID)
    title_prop = prop_map["Title"]
    title_val  = meta.get("title") or os.path.splitext(os.path.basename(file_hint))[0]

    props = {
        title_prop: {"title": [{"text": {"content": s(title_val)}}]},
        prop_map["Tag"]: {"multi_select": [{"name": x.strip()} for x in (meta.get("tag") or [])] if isinstance(meta.get("tag"), (list,tuple)) else [{"name": t.strip()} for t in str(meta.get("tag") or "").split(",") if t.strip()]},
        prop_map["Authors"]: {"rich_text": [{"text": {"content": s(meta.get("authors"))}}]},
        prop_map["Year"]: {"rich_text": [{"text": {"content": s(meta.get("year"))}}]},
        prop_map["Conference/journal"]: {"rich_text": [{"text": {"content": s(meta.get("conference_journal"))}}]},
        prop_map["One sentence"]: {"rich_text": [{"text": {"content": s(meta.get("one_sentence",""))}}]},
    }

    children=[]
    if paper_url: children.append({"object":"block","type":"bookmark","bookmark":{"url":paper_url}})
    if pdf_block: children.append(pdf_block)

    lead_id=None
    if PROMOTE_FIRST_FIGURE and figures_by_id:
        all_fig_list = [{"id": fid, **rec} for fid, rec in figures_by_id.items()]
        lead_id = pick_first_figure_id(all_fig_list)
        if lead_id and figures_by_id.get(lead_id):
            children.append(figures_by_id[lead_id]["image_block"])

    def _heading(text, level=1):
        key={1:"heading_1",2:"heading_2",3:"heading_3"}[max(1,min(level,3))]
        return {"object":"block","type":key, key:{"rich_text":[{"type":"text","text":{"content":text}}]} }
    def _paragraph_rich(parts): return {"object":"block","type":"paragraph","paragraph":{"rich_text":parts}}

    def md_to_rich_text_local(md): return md_to_rich_text(md)

    for item in layout:
        if item.get("type")=="section":
            key=item.get("key"); body=s(contents.get(key,"")).strip()
            if not body: continue
            children.append(_heading(key,1))
            rich=md_to_rich_text_local(body)
            if rich: children.append(_paragraph_rich(rich))
        elif item.get("type")=="figure":
            fid=item.get("id"); rec=figures_by_id.get(fid)
            if rec:
                if PROMOTE_FIRST_FIGURE and lead_id and fid==lead_id: continue
                children.append(rec["image_block"])

    used_keys={i.get("key") for i in layout if i.get("type")=="section"}
    for key in SECTION_KEYS:
        if key not in used_keys and s(contents.get(key,"")).strip():
            children.append(_heading(key,1))
            children.append(_paragraph_rich(md_to_rich_text_local(contents[key])))

    return notion.pages.create(parent={"database_id": DATABASE_ID}, properties=props, children=children)

# â”€â”€ íŒŒì´í”„ë¼ì¸
def process_pdf(path: str):
    print(f"ğŸ“„ Processing: {path}")
    text = strip_unpaired_surrogates(read_pdf(path))
    print(text)
    try: meta = call_gpt_meta(text)
    except Exception as e:
        print(f"âš ï¸ GPT(meta) ì˜¤ë¥˜: {e}"); meta = {"title":"","authors":"","year":"","conference_journal":"","tag":[]}
    if not s(meta.get("year")):
        g=safe_year_guess(os.path.basename(path), text)
        if g: meta["year"]=g

    try: contents = call_gpt_contents_marked(text)
    except Exception as e:
        print(f"âš ï¸ GPT(contents) ì˜¤ë¥˜: {e}"); contents={k:"" for k in SECTION_KEYS}

    try: meta["one_sentence"] = call_gpt_one_sentence(text)
    except Exception: meta["one_sentence"]=""

    try: paper_url = find_paper_url(meta.get("title",""), meta.get("authors",""), meta.get("year",""))
    except Exception: paper_url=""

    pdf_block=None
    try:
        created = create_file_upload(os.path.basename(path), "application/pdf"); fid=created.get("id")
        sent = send_file_upload_path(fid, path, "application/pdf")
        if sent.get("status")=="uploaded": pdf_block = file_block_from_upload(fid)
    except Exception as e:
        print(f"âš ï¸ PDF ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

    try: figures = extract_and_upload_figures(path)
    except Exception as e:
        print(f"âš ï¸ Figure ì¶”ì¶œ/ì—…ë¡œë“œ ì‹¤íŒ¨: {e}"); figures=[]
    figures_by_id = {f["id"]: f for f in figures}

    try: layout = call_gpt_layout(contents, figures)
    except Exception as e:
        print(f"âš ï¸ ë ˆì´ì•„ì›ƒ ìƒì„± ì˜¤ë¥˜: {e}")
        base=[{"type":"section","key":k} for k in SECTION_KEYS]
        m=len(figures); left=[{"type":"figure","id":f["id"]} for f in figures[:m//2]]
        right=[{"type":"figure","id":f["id"]} for f in figures[m//2:]]
        layout=[]
        for it in base:
            layout.append(it)
            if it["key"]=="5. ì œì•ˆ ì‹œìŠ¤í…œ/ë°©ë²•": layout.extend(left)
            if it["key"]=="6. ì‹¤í—˜ ê°€ì„¤/ì ˆì°¨": layout.extend(right)

    for k in SECTION_KEYS:
        print("LEN", k, len(s(contents.get(k, ""))))
    try:
        res = create_notion_page_with_layout(meta, contents, layout, figures_by_id, paper_url, pdf_block, file_hint=path)
        print(f"âœ… Added to Notion: {res.get('url')}\n")
    except APIResponseError as e:
        print(f"âŒ Notion API ì˜¤ë¥˜: {e}\n")

def main():
    pdf_dir = os.path.join(os.getcwd(), "paper")
    if not os.path.isdir(pdf_dir):
        print(f"âŒ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {pdf_dir}"); sys.exit(1)
    pdfs = sorted(glob.glob(os.path.join(pdf_dir, "*.pdf")))
    if not pdfs:
        print("âŒ paper í´ë”ì— PDFê°€ ì—†ìŠµë‹ˆë‹¤."); sys.exit(1)
    for p in pdfs: process_pdf(p)

if __name__ == "__main__":
    main()
